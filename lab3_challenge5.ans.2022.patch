From f26c16b53f7c4d3386aa2d3be694d64c64c330b5 Mon Sep 17 00:00:00 2001
From: ldx123000 <771484893@qq.com>
Date: Wed, 24 Aug 2022 14:07:47 +0000
Subject: [PATCH] finish lab3_chellenge5

---
 kernel/process.c | 23 +++++++++++++++++++++++
 kernel/process.h |  6 ++++++
 kernel/strap.c   | 28 +++++++++++++++++++++++-----
 kernel/vmm.c     |  2 +-
 4 files changed, 53 insertions(+), 6 deletions(-)

diff --git a/kernel/process.c b/kernel/process.c
index 776ff47..e2c690c 100644
--- a/kernel/process.c
+++ b/kernel/process.c
@@ -167,6 +167,7 @@ int free_process( process* proc ) {
 // segments (code, system) of the parent to child. the stack segment remains unchanged
 // for the child.
 //
+ref_pa data_pa[10];
 int do_fork( process* parent)
 {
   sprint( "will fork a child from parent %d.\n", parent->pid );
@@ -210,6 +211,28 @@ int do_fork( process* parent)
         child->mapped_info[child->total_mapped_region].seg_type = CODE_SEGMENT;
         child->total_mapped_region++;
         break;
+      case DATA_SEGMENT:
+        child->mapped_info[i].va = parent->mapped_info[i].va;
+        for (int j = 0; j < parent->mapped_info[i].npages; j++) {
+          uint64 va = parent->mapped_info[i].va + PGSIZE * j;
+          void *pa = (void *)lookup_pa(parent->pagetable, va);
+          for (int i = 0; i < 10; i++) {
+            if (data_pa[i].pa == NULL) {
+              data_pa[i].pa = pa;
+              data_pa[i].ref = 2;
+            } else if (data_pa[i].pa == pa)
+              data_pa[i].ref++;
+          }
+          user_vm_unmap((pagetable_t)parent->pagetable, va, PGSIZE, 0);
+          user_vm_map((pagetable_t)parent->pagetable, va, PGSIZE, (uint64)pa, prot_to_type(PROT_READ, 1));
+          user_vm_map((pagetable_t)child->pagetable, va, PGSIZE, (uint64)pa, prot_to_type(PROT_READ, 1));
+        }
+        child->mapped_info[child->total_mapped_region].va = parent->mapped_info[i].va;
+        child->mapped_info[child->total_mapped_region].npages =
+            parent->mapped_info[i].npages;
+        child->mapped_info[child->total_mapped_region].seg_type = DATA_SEGMENT;
+        child->total_mapped_region++;
+        break;
     }
   }
 
diff --git a/kernel/process.h b/kernel/process.h
index e8ee590..2c57ee9 100644
--- a/kernel/process.h
+++ b/kernel/process.h
@@ -73,6 +73,12 @@ typedef struct process_t {
   int tick_count;
 }process;
 
+//pa refrence
+typedef struct ref_pa{
+  void* pa;
+  int ref;
+}ref_pa;
+
 // switch to run user app
 void switch_to(process*);
 
diff --git a/kernel/strap.c b/kernel/strap.c
index 2f7d4ce..d1109ea 100644
--- a/kernel/strap.c
+++ b/kernel/strap.c
@@ -10,6 +10,7 @@
 #include "vmm.h"
 #include "sched.h"
 #include "util/functions.h"
+#include "string.h"
 
 #include "spike_interface/spike_utils.h"
 
@@ -52,20 +53,37 @@ void handle_mtimer_trap() {
 // sepc: the pc when fault happens;
 // stval: the virtual address that causes pagefault when being accessed.
 //
+extern ref_pa data_pa[10];
 void handle_user_page_fault(uint64 mcause, uint64 sepc, uint64 stval) {
   sprint("handle_page_fault: %lx\n", stval);
+  int flag = 1;
   switch (mcause) {
     case CAUSE_STORE_PAGE_FAULT:
       // TODO (lab2_3): implement the operations that solve the page fault to
       // dynamically increase application stack.
       // hint: first allocate a new physical page, and then, maps the new page to the
       // virtual address that causes the page fault.
-      {
-      uint64 newpage = (uint64)alloc_page();
-      user_vm_map((pagetable_t)current->pagetable, ROUNDDOWN(stval, PGSIZE), PGSIZE, newpage,
-             prot_to_type(PROT_WRITE | PROT_READ, 1));
+      for (int i = 0; i < 10; i++) {
+        if (data_pa[i].pa == (void *)lookup_pa(current->pagetable, stval), PGSIZE) {
+          if (data_pa[i].ref == 1) {
+            void *pa = (void *)lookup_pa(current->pagetable, stval);
+            user_vm_unmap((pagetable_t)current->pagetable, stval, PGSIZE, 0);
+            user_vm_map((pagetable_t)current->pagetable, stval, PGSIZE, (uint64)pa, prot_to_type(PROT_WRITE | PROT_READ, 1));
+          } else {
+            data_pa[i].ref--;
+            void *pa = alloc_page();
+            memcpy(pa, (void *)lookup_pa(current->pagetable, stval), PGSIZE);
+            user_vm_unmap((pagetable_t)current->pagetable, stval, PGSIZE, 0);
+            user_vm_map((pagetable_t)current->pagetable, stval, PGSIZE, (uint64)pa, prot_to_type(PROT_WRITE | PROT_READ, 1));
+          }
+          flag = 0;
+          break;
+        }
+      }
+      if (flag) {
+        map_pages((pagetable_t)current->pagetable, stval, 1, (uint64)alloc_page(),
+                  prot_to_type(PROT_WRITE | PROT_READ, 1));
       }
-
       break;
     default:
       sprint("unknown page fault.\n");
diff --git a/kernel/vmm.c b/kernel/vmm.c
index 5647d8f..7376d38 100644
--- a/kernel/vmm.c
+++ b/kernel/vmm.c
@@ -192,7 +192,7 @@ void user_vm_unmap(pagetable_t page_dir, uint64 va, uint64 size, int free) {
 
   pte_t *pte;
 
-  if ((va % PGSIZE) != 0) panic("uvmunmap: not aligned");
+  //if ((va % PGSIZE) != 0) panic("uvmunmap: not aligned");
 
   for (uint64 a = va; a < va + size; a += PGSIZE) {
     if ((pte = page_walk(page_dir, a, 0)) == 0) panic("uvmunmap: walk");
-- 
2.30.2

